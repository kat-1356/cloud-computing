ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ wget http://172.1.14.200:81/cclab/ccbd-lab/Dataset-2024/insurance_data.csv
--2025-04-07 10:34:33--  http://172.1.14.200:81/cclab/ccbd-lab/Dataset-2024/insurance_data.csv
Connecting to 172.1.14.200:81... connected.
HTTP request sent, awaiting response... 200 OK
Length: 2676 (2.6K) [application/octet-stream]
Saving to: ‘insurance_data.csv.1’

insurance_data.csv.1        100%[==========================================>]   2.61K  --.-KB/s    in 0s      

2025-04-07 10:34:33 (659 MB/s) - ‘insurance_data.csv.1’ saved [2676/2676]

ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat > InsuranceMapper.java << EOL
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class InsuranceMapper extends Mapper<LongWritable, Text, Text, FloatWritable> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] fields = line.split(","); // Adjust delimiter if needed
        if (fields.length > 2) {
            try {
                String region = fields[0]; // Extract region
                float claim = Float.parseFloat(fields[2]); // Extract claim amount
                context.write(new Text(region), new FloatWritable(claim));
            } catch (NumberFormatException e) {
                // Skip invalid rows
            }
        }
    }
}
EOL
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat > InsuranceReducer.java << EOL
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class InsuranceReducer extends Reducer<Text, FloatWritable, Text, FloatWritable> {
    @Override
    protected void reduce(Text key, Iterable<FloatWritable> values, Context context) throws IOException, InterruptedException {
        float maxClaim = Float.MIN_VALUE;
        for (FloatWritable value : values) {
            maxClaim = Math.max(maxClaim, value.get());
        }
        context.write(key, new FloatWritable(maxClaim));
    }
}
EOL
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat > InsuranceDriver.java << EOL
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class InsuranceDriver {
    public static void main(String[] args) throws Exception {
        if (args.length < 2) {
            System.err.println("Usage: InsuranceDriver <input path> <output path>");
            System.exit(-1);
        }

        Job job = Job.getInstance();
        job.setJarByClass(InsuranceDriver.class);
        job.setJobName("Insurance Data Analysis");

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.setMapperClass(InsuranceMapper.class);
        job.setReducerClass(InsuranceReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(FloatWritable.class);

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
EOL
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ javac -classpath $(hadoop classpath) -d . InsuranceMapper.java InsuranceReducer.java InsuranceDriver.java
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ jar -cvf insurance_analysis.jar *.class
added manifest
adding: CropDriver.class(in = 1385) (out= 792)(deflated 42%)
adding: CropMapper.class(in = 1694) (out= 676)(deflated 60%)
adding: DriverExample.class(in = 1396) (out= 796)(deflated 42%)
adding: EarthquakeDriver.class(in = 1411) (out= 798)(deflated 43%)
adding: EarthquakeMapper.class(in = 1712) (out= 685)(deflated 59%)
adding: EmployeesDriver.class(in = 1468) (out= 823)(deflated 43%)
adding: EmployeesMapper.class(in = 1788) (out= 727)(deflated 59%)
adding: EmployeesReducer.class(in = 1684) (out= 715)(deflated 57%)
adding: InsuranceDriver.class(in = 1466) (out= 827)(deflated 43%)
adding: InsuranceMapper.class(in = 1788) (out= 725)(deflated 59%)
adding: InsuranceReducer.class(in = 1684) (out= 714)(deflated 57%)
adding: MapperExample.class(in = 1700) (out= 680)(deflated 60%)
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ hadoop jar insurance_analysis.jar InsuranceDriver file:///home/ritlab-01/1ms22cs60/hadoop-3.2.2/insurance_data.csv file:///home/ritlab-01/1ms22cs60/hadoop-3.2.2/output
2025-04-07 10:35:06,970 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2025-04-07 10:35:07,005 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2025-04-07 10:35:07,005 INFO impl.MetricsSystemImpl: JobTracker metrics system started
Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/output already exists
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:164)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:277)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1565)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1562)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1562)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1583)
	at InsuranceDriver.main(InsuranceDriver.java:27)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat > process_insurance_output.py << EOL
def process_hadoop_output(file_path):
    try:
        with open(file_path, "r") as file:
            lines = file.readlines()

            if not lines:
                print("The output file is empty!")
                return

            print("Hadoop Output:")
            for line in lines:
                print(line.strip())

    except FileNotFoundError:
        print(f"Error: The file '{file_path}' does not exist!")
    except Exception as e:
        print(f"An error occurred: {e}")

output_file = "/home/ritlab-01/1ms22cs60/hadoop-3.2.2/output/part-00000"

process_hadoop_output(output_file)
EOL
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ python3 process_insurance_output.py
Error: The file '/home/ritlab-01/1ms22cs60/hadoop-3.2.2/output/part-00000' does not exist!
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ ls /home/ritlab-01/1ms22cs60/hadoop-3.2.2/output
part-r-00000  _SUCCESS
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ rm -r /home/ritlab-01/1ms22cs60/hadoop-3.2.2/output

hadoop jar insurance_analysis.jar InsuranceDriver file:///home/ritlab-01/1ms22cs60/hadoop-3.2.2/insurance_data.csv file:///home/ritlab-01/1ms22cs60/hadoop-3.2.2/output
2025-04-07 10:36:23,691 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2025-04-07 10:36:23,727 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2025-04-07 10:36:23,727 INFO impl.MetricsSystemImpl: JobTracker metrics system started
2025-04-07 10:36:23,751 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2025-04-07 10:36:23,766 INFO input.FileInputFormat: Total input files to process : 1
2025-04-07 10:36:23,774 INFO mapreduce.JobSubmitter: number of splits:1
2025-04-07 10:36:23,824 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local408280627_0001
2025-04-07 10:36:23,824 INFO mapreduce.JobSubmitter: Executing with tokens: []
2025-04-07 10:36:23,870 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
2025-04-07 10:36:23,871 INFO mapreduce.Job: Running job: job_local408280627_0001
2025-04-07 10:36:23,872 INFO mapred.LocalJobRunner: OutputCommitter set in config null
2025-04-07 10:36:23,875 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2025-04-07 10:36:23,875 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-07 10:36:23,876 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2025-04-07 10:36:23,888 INFO mapred.LocalJobRunner: Waiting for map tasks
2025-04-07 10:36:23,888 INFO mapred.LocalJobRunner: Starting task: attempt_local408280627_0001_m_000000_0
2025-04-07 10:36:23,900 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2025-04-07 10:36:23,900 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-07 10:36:23,907 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2025-04-07 10:36:23,909 INFO mapred.MapTask: Processing split: file:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/insurance_data.csv:0+2676
2025-04-07 10:36:23,948 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
2025-04-07 10:36:23,948 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
2025-04-07 10:36:23,948 INFO mapred.MapTask: soft limit at 83886080
2025-04-07 10:36:23,948 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
2025-04-07 10:36:23,948 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
2025-04-07 10:36:23,950 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2025-04-07 10:36:23,953 INFO mapred.LocalJobRunner: 
2025-04-07 10:36:23,953 INFO mapred.MapTask: Starting flush of map output
2025-04-07 10:36:23,957 INFO mapred.Task: Task:attempt_local408280627_0001_m_000000_0 is done. And is in the process of committing
2025-04-07 10:36:23,958 INFO mapred.LocalJobRunner: map
2025-04-07 10:36:23,958 INFO mapred.Task: Task 'attempt_local408280627_0001_m_000000_0' done.
2025-04-07 10:36:23,960 INFO mapred.Task: Final Counters for attempt_local408280627_0001_m_000000_0: Counters: 17
	File System Counters
		FILE: Number of bytes read=13733
		FILE: Number of bytes written=554488
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=24
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=127
		Combine input records=0
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=260046848
	File Input Format Counters 
		Bytes Read=2676
2025-04-07 10:36:23,960 INFO mapred.LocalJobRunner: Finishing task: attempt_local408280627_0001_m_000000_0
2025-04-07 10:36:23,960 INFO mapred.LocalJobRunner: map task executor complete.
2025-04-07 10:36:23,962 INFO mapred.LocalJobRunner: Waiting for reduce tasks
2025-04-07 10:36:23,962 INFO mapred.LocalJobRunner: Starting task: attempt_local408280627_0001_r_000000_0
2025-04-07 10:36:23,966 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2
2025-04-07 10:36:23,966 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2025-04-07 10:36:23,967 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
2025-04-07 10:36:23,968 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2a059e8
2025-04-07 10:36:23,968 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!
2025-04-07 10:36:23,977 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2874356480, maxSingleShuffleLimit=718589120, mergeThreshold=1897075328, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2025-04-07 10:36:23,979 INFO reduce.EventFetcher: attempt_local408280627_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2025-04-07 10:36:24,001 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local408280627_0001_m_000000_0 decomp: 2 len: 6 to MEMORY
2025-04-07 10:36:24,002 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local408280627_0001_m_000000_0
2025-04-07 10:36:24,003 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
2025-04-07 10:36:24,004 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
2025-04-07 10:36:24,005 INFO mapred.LocalJobRunner: 1 / 1 copied.
2025-04-07 10:36:24,005 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2025-04-07 10:36:24,008 INFO mapred.Merger: Merging 1 sorted segments
2025-04-07 10:36:24,008 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes
2025-04-07 10:36:24,009 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit
2025-04-07 10:36:24,009 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk
2025-04-07 10:36:24,009 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
2025-04-07 10:36:24,009 INFO mapred.Merger: Merging 1 sorted segments
2025-04-07 10:36:24,010 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes
2025-04-07 10:36:24,010 INFO mapred.LocalJobRunner: 1 / 1 copied.
2025-04-07 10:36:24,012 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2025-04-07 10:36:24,014 INFO mapred.Task: Task:attempt_local408280627_0001_r_000000_0 is done. And is in the process of committing
2025-04-07 10:36:24,014 INFO mapred.LocalJobRunner: 1 / 1 copied.
2025-04-07 10:36:24,015 INFO mapred.Task: Task attempt_local408280627_0001_r_000000_0 is allowed to commit now
2025-04-07 10:36:24,015 INFO output.FileOutputCommitter: Saved output of task 'attempt_local408280627_0001_r_000000_0' to file:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/output
2025-04-07 10:36:24,016 INFO mapred.LocalJobRunner: reduce > reduce
2025-04-07 10:36:24,016 INFO mapred.Task: Task 'attempt_local408280627_0001_r_000000_0' done.
2025-04-07 10:36:24,016 INFO mapred.Task: Final Counters for attempt_local408280627_0001_r_000000_0: Counters: 24
	File System Counters
		FILE: Number of bytes read=13777
		FILE: Number of bytes written=554502
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=6
		Reduce input records=0
		Reduce output records=0
		Spilled Records=0
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=260046848
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=8
2025-04-07 10:36:24,016 INFO mapred.LocalJobRunner: Finishing task: attempt_local408280627_0001_r_000000_0
2025-04-07 10:36:24,016 INFO mapred.LocalJobRunner: reduce task executor complete.
2025-04-07 10:36:24,874 INFO mapreduce.Job: Job job_local408280627_0001 running in uber mode : false
2025-04-07 10:36:24,874 INFO mapreduce.Job:  map 100% reduce 100%
2025-04-07 10:36:24,875 INFO mapreduce.Job: Job job_local408280627_0001 completed successfully
2025-04-07 10:36:24,879 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=27510
		FILE: Number of bytes written=1108990
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=24
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=127
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=6
		Reduce input records=0
		Reduce output records=0
		Spilled Records=0
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=520093696
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=2676
	File Output Format Counters 
		Bytes Written=8
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ hadoop jar insurance_analysis.jar InsuranceDriver file:///home/ritlab-01/1ms22cs60/hadoop-3.2.2/insurance_data.csv file:///home/ritlab-01/1ms22cs60/hadoop-3.2.2/output
2025-04-07 10:36:30,455 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
2025-04-07 10:36:30,492 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2025-04-07 10:36:30,492 INFO impl.MetricsSystemImpl: JobTracker metrics system started
Exception in thread "main" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/output already exists
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:164)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:277)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1565)
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1562)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1562)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1583)
	at InsuranceDriver.main(InsuranceDriver.java:27)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat /home/ritlab-01/1ms22cs60/hadoop-3.2.2/insurance_data.csv
101381,FL,ALACHUA COUNTY,186495.3,186495.3,0,0,186495.3,160628.4,0,0,0,0,29.65985,-82.470075,Residential,Wood,3
101761,FL,ALACHUA COUNTY,0,3612725.1,0,3612725.1,3612725.1,4314569.21,0,0,0,0,29.679979,-82.430305,Commercial,Reinforced Masonry,1
102109,FL,ALACHUA COUNTY,12600,12600,12600,12600,12600,17442.43,0,252,0,0,29.66912,-82.34123,Residential,Wood,3
103067,FL,ALACHUA COUNTY,3825,3825,3825,3825,3825,6515.96,0,76.5,0,0,29.61018,-82.42043,Residential,Wood,1
104015,FL,ALACHUA COUNTY,18450000,18450000,18450000,18450000,18450000,19530000,0,0,0,0,29.659702,-82.412109,Commercial,Reinforced Concrete,3
104307,FL,ALACHUA COUNTY,9540,9540,9540,9540,9540,14376.4,0,0,0,0,29.7077,-82.3979,Residential,Wood,4
104374,FL,ALACHUA COUNTY,450,450,450,450,450,443.61,0,9,0,0,29.64186,-82.61538,Residential,Wood,1
106068,FL,ALACHUA COUNTY,450,450,450,450,450,529.95,0,9,0,0,29.66433,-82.37083,Residential,Wood,1
106447,FL,ALACHUA COUNTY,0,245775.8,0,0,245775.8,275679.34,0,0,0,0,29.83628,-82.58826,Residential,Wood,4
107182,FL,ALACHUA COUNTY,0,50787,0,50787,50787,61960.14,0,0,0,0,29.679979,-82.430305,Residential,Wood,1
108397,FL,ALACHUA COUNTY,1800,1800,1800,1800,1800,2146.18,0,36,0,0,29.67887,-82.40204,Residential,Wood,1
110174,FL,ALACHUA COUNTY,450,450,450,450,450,503.71,0,9,0,0,29.66064,-82.29745,Residential,Wood,1
112230,FL,ALACHUA COUNTY,2088,2088,2088,2088,2088,2380.72,0,0,0,0,29.68325,-82.36479,Residential,Wood,1
112920,FL,ALACHUA COUNTY,450,450,450,450,450,673.76,0,9,0,0,29.62885,-82.44461,Residential,Wood,4
114088,FL,ALACHUA COUNTY,1558053,1558053,1558053,1558053,1558053,1978153.95,0,0,0,0,29.623005,-81.975615,Residential,Masonry,3
114113,FL,ALACHUA COUNTY,360,360,360,360,360,537.76,0,7.2,0,0,29.67887,-82.40204,Residential,Wood,1
114275,FL,ALACHUA COUNTY,0,19170000,0,0,19170000,19350000,0,0,0,0,30.104597,-81.830841,Commercial,Reinforced Concrete,4
115105,FL,ALACHUA COUNTY,257736.6,257736.6,0,0,257736.6,432997.49,0,0,0,0,29.634666,-82.356575,Residential,Wood,1
115836,FL,ALACHUA COUNTY,0,86823.85,0,0,86823.85,74887.66,0,0,0,0,29.6786,-82.3694,Residential,Wood,4
120617,FL,ALACHUA COUNTY,370749.6,370749.6,0,0,370749.6,361327.37,0,0,0,0,29.72192,-82.36223,Residential,Masonry,1
120667,FL,ALACHUA COUNTY,0,96514.7,0,0,96514.7,132755.96,0,0,0,0,29.59809,-82.09247,Residential,Wood,1
121735,FL,ALACHUA COUNTY,0,5699151.9,0,5699151.9,5699151.9,8607115.66,0,0,0,0,29.679979,-82.430305,Commercial,Reinforced Masonry,1
121963,FL,ALACHUA COUNTY,14400000,14400000,14400000,14400000,14400000,18000000,0,287589.6,0,0,29.64273,-82.3083,Commercial,Reinforced Concrete,3
123476,FL,ALACHUA COUNTY,0,111265.73,0,0,111265.73,182849.1,0,0,0,0,29.6517,-82.3253,Residential,Wood,4
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat > analyze_insurance_direct.py << EOL
import csv
from collections import defaultdict

data_file = "/home/ritlab-01/1ms22cs60/hadoop-3.2.2/insurance_data.csv"
output_file = "/home/ritlab-01/1ms22cs60/hadoop-3.2.2/insurance_direct_output.txt"

def process_insurance_data(input_path, output_path):
    max_claim_by_region = defaultdict(float)

    with open(input_path, "r") as f:
        reader = csv.reader(f, delimiter=",")  # Adjust delimiter if needed
        for row in reader:
            if len(row) > 2:
                try:
                    region = row[0]
                    claim = float(row[2])
                    max_claim_by_region[region] = max(max_claim_by_region[region], claim)
                except ValueError:
                    pass  # Skip invalid rows

    with open(output_path, "w") as f:
        for region, claim in max_claim_by_region.items():
            f.write(f"{region}: {claim}\n")

process_insurance_data(data_file, output_file)
EOL
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ python3 analyze_insurance_direct.py
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat /home/ritlab-01/1ms22cs60/hadoop-3.2.2/insurance_direct_output.txt
Skipping invalid row: ['101761', 'FL', 'ALACHUA COUNTY', '0', '3612725.1', '0', '3612725.1', '3612725.1', '4314569.21', '0', '0', '0', '0', '29.679979', '-82.430305', 'Commercial', 'Reinforced Masonry', '1']
Skipping invalid row: ['102109', 'FL', 'ALACHUA COUNTY', '12600', '12600', '12600', '12600', '12600', '17442.43', '0', '252', '0', '0', '29.66912', '-82.34123', 'Residential', 'Wood', '3']
Skipping invalid row: ['103067', 'FL', 'ALACHUA COUNTY', '3825', '3825', '3825', '3825', '3825', '6515.96', '0', '76.5', '0', '0', '29.61018', '-82.42043', 'Residential', 'Wood', '1']
Skipping invalid row: ['104015', 'FL', 'ALACHUA COUNTY', '18450000', '18450000', '18450000', '18450000', '18450000', '19530000', '0', '0', '0', '0', '29.659702', '-82.412109', 'Commercial', 'Reinforced Concrete', '3']

