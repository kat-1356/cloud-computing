ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ wget http://172.1.14.200:81/cclab/ccbd-lab/Dataset-2024/crop_production.csv
--2025-04-07 10:11:31--  http://172.1.14.200:81/cclab/ccbd-lab/Dataset-2024/crop_production.csv
Connecting to 172.1.14.200:81... connected.
HTTP request sent, awaiting response... 200 OK
Length: 15316741 (15M) [application/octet-stream]
Saving to: ‘crop_production.csv’

crop_production.csv         100%[==========================================>]  14.61M  97.1MB/s    in 0.2s    

2025-04-07 10:11:31 (97.1 MB/s) - ‘crop_production.csv’ saved [15316741/15316741]

ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ mv crop_production.csv ~/1ms22cs60/hadoop-3.2.2/crop_production.csv
mv: 'crop_production.csv' and '/home/ritlab-01/1ms22cs60/hadoop-3.2.2/crop_production.csv' are the same file
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ echo 'import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class CropMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] fields = line.split(","); // Adjust delimiter based on your file format
        if (fields.length > 1) {
            String cropName = fields[0]; // Extract crop name
            int production = Integer.parseInt(fields[1]); // Extract production quantity
            context.write(new Text(cropName), new IntWritable(production));
        }
    }
}' > CropMapper.java
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ echo 'import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class CropDriver {
    public static void main(String[] args) throws Exception {
        if (args.length < 2) {
            System.err.println("Usage: CropDriver <input path> <output path>");
            System.exit(-1);
        }

        Job job = Job.getInstance();
        job.setJarByClass(CropDriver.class);
        job.setJobName("Crop Production Analysis");

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.setMapperClass(CropMapper.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}' > CropDriver.java
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ javac -classpath $(hadoop classpath) -d . CropMapper.java CropDriver.java
ERROR: JAVA_HOME is not set and could not be found.
error: invalid flag: .
Usage: javac <options> <source files>
use --help for a list of possible options
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ export JAVA_HOME=$(readlink -f $(which javac) | awk 'BEGIN {FS="/bin"} {print $1}')
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ echo $JAVA_HOME
/usr/lib/jvm/java-11-openjdk-amd64
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ export PATH=$JAVA_HOME/bin:$PATH
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ echo $PATH
/usr/lib/jvm/java-11-openjdk-amd64/bin:/home/ritlab-01/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/bin
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ echo 'export JAVA_HOME=$(readlink -f $(which javac) | awk "BEGIN {FS=\"/bin\"} {print \$1}")' >> ~/.bashrc
echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ javac -classpath $(hadoop classpath) -d . CropMapper.java CropDriver.java
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ hadoop classpath
/home/ritlab-01/1ms22cs60/hadoop-3.2.2/etc/hadoop:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/share/hadoop/common/lib/*:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/share/hadoop/common/*:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/share/hadoop/hdfs:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/share/hadoop/hdfs/lib/*:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/share/hadoop/hdfs/*:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/share/hadoop/mapreduce/lib/*:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/share/hadoop/mapreduce/*:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/share/hadoop/yarn:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/share/hadoop/yarn/lib/*:/home/ritlab-01/1ms22cs60/hadoop-3.2.2/share/hadoop/yarn/*
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ jar -cvf crop_analysis.jar -C classes .
hadoop jar crop_analysis.jar CropDriver file:///path/to/crop_production.csv file:///path/to/output
classes/. : no such file or directory
JAR does not exist or is not a normal file: /home/ritlab-01/1ms22cs60/hadoop-3.2.2/crop_analysis.jar
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ mkdir classes
javac -classpath $(hadoop classpath) -d classes CropMapper.java CropDriver.java
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ ls ~/1ms22cs60/hadoop-3.2.2/output
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat ~/1ms22cs60/hadoop-3.2.2/output/part-00000
cat: /home/ritlab-01/1ms22cs60/hadoop-3.2.2/output/part-00000: No such file or directory
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ chmod -R 755 ~/1ms22cs60/hadoop-3.2.2/output
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ hadoop jar crop_analysis.jar CropDriver file:///home/ritlab-01/1ms22cs60/hadoop-3.2.2/crop_production.csv file:///home/ritlab-01/1ms22cs60/hadoop-3.2.2/output
JAR does not exist or is not a normal file: /home/ritlab-01/1ms22cs60/hadoop-3.2.2/crop_analysis.jar
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat > analyze_weather.py << EOL
import csv
from collections import defaultdict

data_file = "/home/ritlab-01/1ms22cs60/hadoop-3.2.2/weather_report.txt"
output_file = "/home/ritlab-01/1ms22cs60/hadoop-3.2.2/weather_output.txt"

def process_weather_data(input_path, output_path):
    max_temp_by_date = defaultdict(int)

    # Open the dataset
    with open(input_path, "r") as f:
        reader = csv.reader(f)
        for row in reader:
            if len(row) > 1:
                try:
                    date = row[0]
                    temperature = int(row[1])
                    max_temp_by_date[date] = max(max_temp_by_date[date], temperature)
                except ValueError:
                    # Skip invalid rows
                    pass

    # Write results to a file
    with open(output_path, "w") as f:
        for date, temperature in max_temp_by_date.items():
            f.write(f"{date}: {temperature}\n")

process_weather_data(data_file, output_file)
EOL
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ python3 analyze_weather.py
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat /home/ritlab-01/1ms22cs60/hadoop-3.2.2/weather_output.txt
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat > analyze_crop.py << EOL
import csv
from collections import defaultdict

data_file = "/home/ritlab-01/1ms22cs60/hadoop-3.2.2/crop_production.csv"
output_file = "/home/ritlab-01/1ms22cs60/hadoop-3.2.2/crop_output.txt"

def process_crop_data(input_path, output_path):
    max_production_by_crop = defaultdict(int)

    # Open the dataset
    with open(input_path, "r") as f:
        reader = csv.reader(f)
        for row in reader:
            if len(row) > 1:
                try:
                    crop = row[0]
                    production = int(row[1])
                    max_production_by_crop[crop] = max(max_production_by_crop[crop], production)
                except ValueError:
                    # Skip invalid rows
                    pass

    # Write results to a file
    with open(output_path, "w") as f:
        for crop, production in max_production_by_crop.items():
            f.write(f"{crop}: {production}\n")

process_crop_data(data_file, output_file)
EOL
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ python3 analyze_crop.py
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat /home/ritlab-01/1ms22cs60/hadoop-3.2.2/crop_output.txt
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat > analyze_earthquake.py << EOL
import csv
from collections import defaultdict

data_file = "/home/ritlab-01/1ms22cs60/hadoop-3.2.2/earthquake_data.csv"
output_file = "/home/ritlab-01/1ms22cs60/hadoop-3.2.2/earthquake_output.txt"

def process_earthquake_data(input_path, output_path):
    max_magnitude_by_location = defaultdict(float)

    # Open the dataset
    with open(input_path, "r") as f:
        reader = csv.reader(f)
        for row in reader:
            print(row)  # Debugging line to show each row being processed
            if len(row) > 1:
                try:
                    location = row[0]  # Extract location
                    magnitude = float(row[1])  # Extract magnitude
                    max_magnitude_by_location[location] = max(max_magnitude_by_location[location], magnitude)
                except ValueError:
                    print(f"Skipping invalid row: {row}")  # Debug invalid rows

    # Write results to a file
    with open(output_path, "w") as f:
        for location, magnitude in max_magnitude_by_location.items():
            f.write(f"{location}: {magnitude}\n")

process_earthquake_data(data_file
> 
> ^C
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat > analyze_earthquake.py << EOL
import csv
from collections import defaultdict

data_file = "/home/ritlab-01/1ms22cs60/hadoop-3.2.2/earthquake_data.csv"
output_file = "/home/ritlab-01/1ms22cs60/hadoop-3.2.2/earthquake_output.txt"

def process_earthquake_data(input_path, output_path):
    max_magnitude_by_location = defaultdict(float)

    # Open the dataset
    with open(input_path, "r") as f:
        reader = csv.reader(f)
        for row in reader:
            print(row)  # Debugging line to show each row being processed
            if len(row) > 1:
                try:
                    location = row[0]  # Extract location
                    magnitude = float(row[1])  # Extract magnitude
                    max_magnitude_by_location[location] = max(max_magnitude_by_location[location], magnitude)
                except ValueError:
                    print(f"Skipping invalid row: {row}")  # Debug invalid rows

    # Write results to a file
    with open(output_path, "w") as f:
        for location, magnitude in max_magnitude_by_location.items():
            f.write(f"{location}: {magnitude}\n")

process_earthquake_data(data_file, output_file)
EOL
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ python3 analyze_earthquake.py
['ak', '10636675', '2', 'Saturday, January  5, 2013 20:11:13 UTC', '51.7622', '-175.8648', '8.9', '59.4', '9', 'Acme Islands, FOR TEST PURPOSES']
['us', 'b000em0n', '7', 'Friday, January 11, 2013 00:30:21 UTC', '39.6307', '25.5102', '4.3', '5.8', '73', 'Aegean Sea']
Skipping invalid row: ['us', 'b000em0n', '7', 'Friday, January 11, 2013 00:30:21 UTC', '39.6307', '25.5102', '4.3', '5.8', '73', 'Aegean Sea']
['us', '2013kea6', 'A', 'Wednesday, January  9, 2013 15:41:33 UTC', '39.7105', '25.6298', '4.4', '9', '90', 'Aegean Sea']
Skipping invalid row: ['us', '2013kea6', 'A', 'Wednesday, January  9, 2013 15:41:33 UTC', '39.7105', '25.6298', '4.4', '9', '90', 'Aegean Sea']
['us', 'c000el2f', 'D', 'Tuesday, January  8, 2013 14:16:08 UTC', '39.658', '25.543', '5.7', '10.3', '500', 'Aegean Sea']
Skipping invalid row: ['us', 'c000el2f', 'D', 'Tuesday, January  8, 2013 14:16:08 UTC', '39.658', '25.543', '5.7', '10.3', '500', 'Aegean Sea']
['ak', '10637362', '1', 'Saturday, January 12, 2013 09:40:45 UTC', '57.8248', '-155.6121', '2', '13.5', '6', 'Alaska Peninsula']
['ak', '10635810', '2', 'Thursday, January 10, 2013 13:18:04 UTC', '58.4197', '-156.5708', '3.1', '201.5', '64', 'Alaska Peninsula']
['ak', '10636342', '2', 'Wednesday, January  9, 2013 06:25:10 UTC', '58.8165', '-154.6988', '2.4', '151.4', '18', 'Alaska Peninsula']
['ak', '10631962', '2', 'Sunday, January  6, 2013 00:08:17 UTC', '54.4668', '-161.6908', '3', '17', '14', 'Alaska Peninsula']
['us', 'b000em8m', '9', 'Friday, January 11, 2013 09:10:15 UTC', '13.349', '92.3832', '5', '34.2', '64', 'Andaman Islands, India region']
Skipping invalid row: ['us', 'b000em8m', '9', 'Friday, January 11, 2013 09:10:15 UTC', '13.349', '92.3832', '5', '34.2', '64', 'Andaman Islands, India region']
['ak', '10637352', '2', 'Saturday, January 12, 2013 08:24:29 UTC', '51.2826', '-178.4394', '2.7', '23.2', '12', 'Andreanof Islands, Aleutian Islands, Alaska']
['ak', '10637232', '2', 'Saturday, January 12, 2013 00:43:43 UTC', '52.1313', '-174.7922', '2.2', '96.6', '13', 'Andreanof Islands, Aleutian Islands, Alaska']
['ak', '10632070', '2', 'Sunday, January  6, 2013 09:21:21 UTC', '51.5449', '-178.2224', '1.6', '32.2', '15', 'Andreanof Islands, Aleutian Islands, Alaska']
['ak', '10633473', '2', 'Sunday, January  6, 2013 07:07:20 UTC', '51.399', '-174.3559', '1.8', '10.1', '7', 'Andreanof Islands, Aleutian Islands, Alaska']
['us', 'c000eltc', 'I', 'Wednesday, January  9, 2013 11:21:18 UTC', '-24.268', '-69.522', '5.1', '115.3', '0', 'Antofagasta, Chile']
Skipping invalid row: ['us', 'c000eltc', 'I', 'Wednesday, January  9, 2013 11:21:18 UTC', '-24.268', '-69.522', '5.1', '115.3', '0', 'Antofagasta, Chile']
['uu', '60009747', '5', 'Monday, January  7, 2013 15:36:43 UTC', '36.8383', '-111.8563', '3.1', '6.4', '18', 'Arizona']
['nm', '010713b', 'A', 'Sunday, January  6, 2013 21:47:17 UTC', '35.8918', '-91.9482', '1.8', '2.1', '8', 'Arkansas']
Skipping invalid row: ['nm', '010713b', 'A', 'Sunday, January  6, 2013 21:47:17 UTC', '35.8918', '-91.9482', '1.8', '2.1', '8', 'Arkansas']
['us', 'c000ekie', '4', 'Monday, January  7, 2013 05:32:44 UTC', '28.325', '94.3088', '4.2', '18.1', '18', 'Arunachal Pradesh, India']
Skipping invalid row: ['us', 'c000ekie', '4', 'Monday, January  7, 2013 05:32:44 UTC', '28.325', '94.3088', '4.2', '18.1', '18', 'Arunachal Pradesh, India']
['us', 'c000ekxw', '5', 'Tuesday, January  8, 2013 02:51:45 UTC', '19.2215', '121.2571', '4.5', '10', '61', 'Babuyan Islands region, Philippines']
Skipping invalid row: ['us', 'c000ekxw', '5', 'Tuesday, January  8, 2013 02:51:45 UTC', '19.2215', '121.2571', '4.5', '10', '61', 'Babuyan Islands region, Philippines']
['ci', '15272145', '0', 'Friday, January 11, 2013 09:04:36 UTC', '32.2308', '-115.242', '2.8', '41.8', '19', 'Baja California, Mexico']
['ci', '15271721', '2', 'Thursday, January 10, 2013 00:43:10 UTC', '32.5373', '-115.6152', '2.1', '12.7', '29', 'Baja California, Mexico']
['ci', '15271457', '2', 'Wednesday, January  9, 2013 02:46:53 UTC', '32.227', '-115.2127', '2.3', '13.8', '15', 'Baja California, Mexico']
['ak', '10633429', '2', 'Wednesday, January  9, 2013 00:31:30 UTC', '64.9952', '-147.3775', '1.5', '0', '12', 'Central Alaska']
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ cat /home/ritlab-01/1ms22cs60/hadoop-3.2.2/earthquake_output.txt
ak: 10637362.0
uu: 60009747.0
ci: 15272145.0
ritlab-01@ritlab01-ThinkCentre-M70t-Gen-3:~/1ms22cs60/hadoop-3.2.2$ 

